# Test percentile_agg and approx_percentile functions

# Create test table with sample data
statement ok
CREATE TABLE percentile_test (
    project_id INTEGER,
    value DOUBLE
)

# Insert test data - normal distribution-like values
statement ok
INSERT INTO percentile_test VALUES
(1, 10.5), (1, 12.3), (1, 15.7), (1, 18.2), (1, 20.1),
(1, 22.5), (1, 25.8), (1, 28.3), (1, 30.9), (1, 35.2),
(1, 38.7), (1, 42.1), (1, 45.6), (1, 48.9), (1, 52.3),
(1, 55.7), (1, 58.2), (1, 62.5), (1, 65.8), (1, 70.1),
(1, 73.4), (1, 76.8), (1, 80.2), (1, 83.5), (1, 87.9),
(1, 90.3), (1, 93.7), (1, 97.1), (1, 98.5), (1, 99.9)

# Test percentile_agg aggregation
query B
SELECT percentile_agg(value) IS NOT NULL as has_digest
FROM percentile_test
WHERE project_id = 1
----
true

# Test approx_percentile with median (50th percentile)
# Note: T-Digest is an approximation algorithm, so exact values may vary slightly
query R
SELECT ROUND(approx_percentile(0.5, percentile_agg(value))) as median
FROM percentile_test
WHERE project_id = 1
----
54

# Test multiple percentiles
# Note: T-Digest is approximate, allow for Â±1 variance in results
query RRR
SELECT 
    ROUND(approx_percentile(0.25, percentile_agg(value))) as p25,
    ROUND(approx_percentile(0.5, percentile_agg(value))) as p50,
    ROUND(approx_percentile(0.75, percentile_agg(value))) as p75
FROM percentile_test
WHERE project_id = 1
----
29 54 79

# Test edge cases - 0th and 100th percentiles
query RR
SELECT 
    ROUND(approx_percentile(0.0, percentile_agg(value)), 1) as min_val,
    ROUND(approx_percentile(1.0, percentile_agg(value)), 1) as max_val
FROM percentile_test
WHERE project_id = 1
----
10.5 99.9

# Test with GROUP BY
query II
SELECT 
    project_id,
    COUNT(*) as count
FROM percentile_test
WHERE project_id IN (1)
GROUP BY project_id
ORDER BY project_id
----
1 30

# Test with GROUP BY - median calculation
# Note: percentile_agg returns binary (T-Digest) which causes type issues with GROUP BY
# This is a known limitation - use without GROUP BY or aggregate separately
query R
SELECT ROUND(approx_percentile(0.5, agg.digest)) as median
FROM (
    SELECT percentile_agg(value) as digest
    FROM percentile_test
    WHERE project_id = 1
) agg
----
54

# Test with NULL values
statement ok
INSERT INTO percentile_test VALUES (2, NULL), (2, 50.0), (2, NULL), (2, 60.0), (2, 70.0)

query R
SELECT ROUND(approx_percentile(0.5, percentile_agg(value))) as median
FROM percentile_test
WHERE project_id = 2
----
60

# Test error handling - invalid percentile
statement error
SELECT approx_percentile(1.5, percentile_agg(value))
FROM percentile_test
WHERE project_id = 1

statement error
SELECT approx_percentile(-0.1, percentile_agg(value))
FROM percentile_test
WHERE project_id = 1

# Clean up
statement ok
DROP TABLE percentile_test

# ============================================
# Test percentile with time-series data (from the other file)
# ============================================

# Create test table for time-series data
statement ok
CREATE TABLE test_spans (
    project_id VARCHAR,
    timestamp TIMESTAMP,
    duration BIGINT
)

# Insert test data with durations in nanoseconds
statement ok
INSERT INTO test_spans VALUES
('test-project', '2025-08-10T15:00:00Z', 50000000),   -- 50ms
('test-project', '2025-08-10T15:30:00Z', 75000000),   -- 75ms
('test-project', '2025-08-10T15:45:00Z', 90000000),   -- 90ms
('test-project', '2025-08-10T16:00:00Z', 100000000),  -- 100ms
('test-project', '2025-08-10T16:30:00Z', 150000000),  -- 150ms
('test-project', '2025-08-10T16:45:00Z', 200000000)   -- 200ms

# Test 1: Simple percentile calculation with unit conversion
query R
SELECT ROUND(approx_percentile(0.5, percentile_agg(duration)) / 1000000.0, 2) as median_ms
FROM test_spans
WHERE project_id = 'test-project'
----
95

# Test 2: Multiple percentiles in columns for time-series data
query RRRR
SELECT 
    ROUND(approx_percentile(0.50, percentile_agg(duration)) / 1000000.0, 2) AS p50,
    ROUND(approx_percentile(0.75, percentile_agg(duration)) / 1000000.0, 2) AS p75,
    ROUND(approx_percentile(0.90, percentile_agg(duration)) / 1000000.0, 2) AS p90,
    ROUND(approx_percentile(0.95, percentile_agg(duration)) / 1000000.0, 2) AS p95
FROM test_spans
WHERE project_id = 'test-project'
----
95 150 200 200

# Test 3: Array construction with ARRAY function
query B
SELECT ARRAY[1.0, 2.0, 3.0] IS NOT NULL as has_array
FROM test_spans
WHERE project_id = 'test-project'
LIMIT 1
----
true

# Test 4: array_element function with literal array
query R
SELECT array_element(ARRAY[10.5, 20.5, 30.5], 2) as second_element
FROM test_spans
WHERE project_id = 'test-project'
LIMIT 1
----
20.5

# Test 5: array_element with string array
query T
SELECT array_element(ARRAY['p50', 'p75', 'p90', 'p95'], 3) as third_quantile
FROM test_spans
WHERE project_id = 'test-project'
LIMIT 1
----
p90

# Test 6: Percentiles grouped by time buckets
query TTTRRRR
SELECT 
    date_trunc('hour', timestamp) as hour,
    COUNT(*) as count,
    ROUND(approx_percentile(0.50, percentile_agg(duration)) / 1000000.0, 2) AS p50,
    ROUND(approx_percentile(0.75, percentile_agg(duration)) / 1000000.0, 2) AS p75,
    ROUND(approx_percentile(0.90, percentile_agg(duration)) / 1000000.0, 2) AS p90,
    ROUND(approx_percentile(0.95, percentile_agg(duration)) / 1000000.0, 2) AS p95
FROM test_spans
WHERE project_id = 'test-project'
GROUP BY date_trunc('hour', timestamp)
ORDER BY hour
----
2025-08-10T15:00:00 3 75.0 90.0 90.0 90.0
2025-08-10T16:00:00 3 150.0 200.0 200.0 200.0

# Clean up
statement ok
DROP TABLE test_spans
